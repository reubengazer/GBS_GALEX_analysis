{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Galex halfs into whole set.\n",
      "Converted longitudes < 360 to negative values.\n",
      "Trimmed Galex observations to represent the GBS area, reducing observations from 372514 to 112825 lines.\n",
      "Renamed columns.\n",
      "Unduplicating ... this may take a few moments ...\n",
      "Removed duplicate Galex observations, reducing observations from 112825 to 88002 lines.\n",
      "The header of the cleaned dataframe:\n",
      "          objid      GLX_RA    GLX_DEC      glon      glat  DIST_2_FOV  \\\n",
      "0  3.734542e+18  264.308162 -27.635378  0.128663  2.254788    0.602792   \n",
      "1  3.734542e+18  264.306933 -27.632841  0.130223  2.257066    0.601209   \n",
      "2  3.734542e+18  264.308695 -27.630853  0.132741  2.256813    0.602340   \n",
      "3  3.734542e+18  264.295083 -27.626682  0.129799  2.269231    0.589694   \n",
      "4  3.734542e+18  264.304047 -27.631153  0.130279  2.260129    0.598364   \n",
      "\n",
      "   NUV_FLUX  NUV_FLUX_ERR    NUV_MAG  NUV_MAG_ERR  \n",
      "0  1.566555      1.130954  23.412636     0.784023  \n",
      "1  1.141192      1.133325  23.756603     1.078514  \n",
      "2  1.626406      1.269061  23.371927     0.847390  \n",
      "3  1.316818      0.803056  23.601185     0.662292  \n",
      "4  1.459011      1.680690  23.489853     1.251004  \n",
      "Saved cleaned data here: dat/galex_CLEAN.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Right and left half of Galex data.\n",
    "# We queired CasJobs which holds the Galex NUV data.\n",
    "# Due to the way the longitude in degrees works, we needed to query \n",
    "# all Galex points in the GBS region, which is continuous section\n",
    "# with longitudes between ~ 357-360 and 0-3 degrees.\n",
    "# Due to this split over 360, we need two queries for right and left halves, then we stack.\n",
    "\n",
    "def stack_halves(right,left):\n",
    "    \"\"\"Stack the right and left halves of the CasJobs-queried Galex data, vertically.\"\"\"\n",
    "    glx = pd.concat([right,left],axis=0)\n",
    "    print('Stacked Galex halfs into whole set.')\n",
    "    return(glx)\n",
    "\n",
    "def convert_glon(df):\n",
    "    \"\"\"Convert longitude values < 360 to negative equivalents where 360 meets 0.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.loc[df['glon']<360,'glon'].apply(lambda long: long-360)\n",
    "    print('Converted longitudes < 360 to negative values.')\n",
    "    return(df)\n",
    "\n",
    "def trim(df):\n",
    "    \"\"\"Trim the Galex observations by removing all points out of the standard GBS region:\n",
    "    We need to cut out the center horizontal chunk not included in the GBS between ~ -1 < lat < +1.\"\"\"\n",
    "    df = df.copy()\n",
    "    len_init = len(df)\n",
    "    long_max,long_min = 3.1, -3.1\n",
    "    df = df[(df['glon'] < long_max) & (df['glon'] > long_min)] # cut longitude.\n",
    "    df = df[(df['glat'] > 0.83) | (df['glat'] < -0.83)] # cut latitude by removing middle.\n",
    "    len_fin = len(df)\n",
    "    print('Trimmed Galex observations to represent the GBS area, reducing observations from {len_init} to {len_fin} lines.'.format(**locals()))\n",
    "    return(df)\n",
    "\n",
    "def rename_cols(df):\n",
    "    \"\"\"Rename the columns to more reasonable names.\"\"\"\n",
    "    df = df.copy()\n",
    "    change_dict = {'nuv_mag':'NUV_MAG','nuv_magerr':'NUV_MAG_ERR','nuv_flux':'NUV_FLUX',\\\n",
    "    'nuv_fluxerr':'NUV_FLUX_ERR','ra':'GLX_RA','dec':'GLX_DEC','fov_radius':'DIST_2_FOV'}\n",
    "    df.rename(columns=change_dict,inplace=True)\n",
    "    print('Renamed columns.')\n",
    "    return(df)\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    \"\"\"Remove actual duplicate observations by some column.\n",
    "    Also remove CLOSE duplicates, observations with very close coordinates (in multiple images).\n",
    "    There are MANY of this class, because the entire dataset is comprised of many non-unique sets.\n",
    "    Initialize real_sources with the first observation.\n",
    "    Loop through the rest of the dataset - if they aren't too close, add them to real_sources.\n",
    "    (The NUV density is much lower than 1 per 10\" radius around any point).\"\"\"\n",
    "    \n",
    "    def is_duplicate(point,df,tol=None):\n",
    "        \"\"\"Return a boolean TRUE or FALSE if there exists an observation in df\n",
    "        that is sufficiently close to 'point'.\n",
    "        - point: an observation from the Galex dataset.\n",
    "        - df: a comparison set.\n",
    "        - tol: distance tolerance to accept or reject a source.\"\"\"\n",
    "        # Set tolerance if not None.\n",
    "        if tol == None: \n",
    "            tol = 0.000694444 # degrees, or 2.5 arcseconds.\n",
    "        # Distance between point and all other observations in df, Euclidean 2-norm.\n",
    "        distance = np.sqrt((point['GLX_RA']-df['GLX_RA'])**2.0 + (point['GLX_DEC']-df['GLX_DEC'])**2.0)\n",
    "        # If any are close, return True.\n",
    "        return(any([dist < tol for dist in distance]))\n",
    "    \n",
    "    df = df.copy()\n",
    "    # Sort df by DIST_2_FOV as they have the smallest observational errors among duplicates.\n",
    "    df.sort_values(by=['DIST_2_FOV'])\n",
    "    # Create empty dataframe for our unduplicated observations.\n",
    "    real_sources = pd.DataFrame(columns=df.columns)\n",
    "    real_sources = real_sources.append(df.iloc[0])\n",
    "    # Loop through data, only select those that have no duplicates within tol.\n",
    "    print('Unduplicating ... this may take a few moments ...')\n",
    "    for i, point in df.drop(0,axis=0).iterrows(): # don't do the 0th observation as this is the initial one!\n",
    "        if is_duplicate(point,real_sources) != True: # if it's unique in the set thus far, no dupe\n",
    "            real_sources = real_sources.append(point)\n",
    "\n",
    "    print('Removed duplicate Galex observations, reducing observations from {} to {} lines.'.format(len(df),len(real_sources)))\n",
    "    return(real_sources)\n",
    "\n",
    "# Filepaths of right and left halves of the Galex data in the rough GBS region.\n",
    "glx_right = pd.read_csv('dat/galex_right_half_RAW.csv')\n",
    "glx_left = pd.read_csv('dat/galex_left_half_RAW.csv')\n",
    "df = stack_halves(glx_right,glx_left)\n",
    "df.to_csv('dat/galex_RAW.csv')\n",
    "\n",
    "# Apply each of the cleaning functions to the dataframe and output the header.\n",
    "df_clean = df.pipe(convert_glon).pipe(trim).pipe(rename_cols).pipe(remove_duplicates)\n",
    "print('The header of the cleaned dataframe:')\n",
    "print(df_clean.head())\n",
    "\n",
    "# Output filepath of cleaned data.\n",
    "fp_out = 'dat/galex_CLEAN.csv'\n",
    "df_clean.to_csv(fp_out)\n",
    "print('Saved cleaned data here: {}'.format(fp_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
